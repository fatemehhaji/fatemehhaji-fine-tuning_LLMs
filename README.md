# Fine-Tuning Large Language Models (LLMs)

## Project Description

This project focuses on fine-tuning Large Language Models (LLMs) such as LLaMa-2, Mistral, and Phi-2 for text generation tasks. The fine-tuning process is conducted using PyTorch and the Hugging Face Transformers library.

## Setup

To set up the project, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/fatemehhaji/fine-tuning_LLMs.git

2. Create a Virtual Environment

   Navigate to the project directory and create a virtual environment:
   
   ```bash
   cd your-repository
   python3 -m venv venv
   ```
3. Install Dependencies

   Install the required dependencies from the `requirements.txt` file:
   
   ```bash
   pip install -r requirements.txt
   ```

4. Run the notebooks:
- `finetune_llama2.ipynb`: Fine-tuning LLaMa-2 model
- `finetune_phi2.ipynb`: Fine-tuning Phi-2 model
- `finetune_mistral.ipynb`: Fine-tuning Mistral model
- `evaluation.ipynb`: Evaluating the fine-tuned models

## Model Details

We fine-tune three different LLMs on a text dataset. The fine-tuning process involves adapting the pre-trained model to generate text based on an instruction. We use the following models:

- LLaMa-2 (7B) [NousResearch/Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf)
- Phi-2 (2.7B) [microsoft/phi-2](https://huggingface.co/microsoft/phi-2)
- Mistral (7B) [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

## Dataset Used

The dataset used for fine-tuning is Alpaca:

- **Alpaca**: [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)

A sample of the dataset is provided in the repository for reference.

| Field        | Value                                                                                                                             |
|--------------|-----------------------------------------------------------------------------------------------------------------------------------|
| instruction  | Give three tips for staying healthy.                                                                                              |
| input        | Below is an instruction that describes a task. Write a response that appropriately completes the request. ###                    |
| output       | 1. Eat a balanced diet and make sure to include plenty of fruits and vegetables. 2. Exercise regularly to keep your body active and strong. 3. Get enough sleep and maintain a consistent sleep schedule. |
| text         | Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Give three tips for staying healthy. ### Response: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. 2. Exercise regularly to keep your body active and strong. 3. Get enough sleep and maintain a consistent sleep schedule.                                                                                         |


## Additional Notes

To run the notebooks, ensure access to a high-RAM GPU. This project utilized a V100 GPU. 
Adjust the parameters according to your setup requirements.

## Metric Measurements

After fine-tuning, we measure various metrics to evaluate the quality of the generated text, including perplexity, BLEU score, ROUGE-L score, and BERTScore. We also conduct a small-scale human evaluation to assess the generated text's grammatical correctness, coherence, and correctness of the answer.

Discussion - The results from the table show that the phi-2-finetuned model slightly outperforms the Llama-2-7b-hf-finetuned and Mistral-7B-finetuned models in terms of BLEU, ROUGE-L, Perplexity, and BERTScore metrics. The BLEU scores are low for all models, suggesting that this metric may not be the best indicator of performance for this specific task. On the other hand, BERTScores are notably higher, indicating a better alignment with semantic similarity in the generated text. Human evaluation scores are also relatively high for all models, suggesting that the generated text is comprehensible to humans. All models seem to create extra words in a similar way, which affects their scores across different tests in the same manner.

Generated using settings: top_k = 50, num_beams = 5, and temperature = 1.

| Model Name                            | BLEU     | ROUGE-L   | BERTScore | Perplexity | Human Evaluation |
|---------------------------------------|----------|-----------|-----------|------------|------------------|
| phi-2-finetuned                       | 0.469636 | 0.604907  | 0.931351  | 19.0088    | 0.895062         |
| Llama-2-7b-hf-finetuned               | 0.137808 | 0.38557   | 0.897052  | 10.0402    | 0.919753         |
| Mistral-7B-finetuned                  | 0.110753 | 0.304393  | 0.855278  | 8.10991    | 0.845679         |

## Human Evaluation Scores

The human-evaluated scores for 27 data points, originally generated by GPT-4 and then validated by human, are available in the following JSON files:

- [`data/section_2/Llama-2-7b-hf-finetuned_topk50_nb5_t1_HE.json`](data/section_2/Llama-2-7b-hf-finetuned_topk50_nb5_t1_HE.json)
- [`data/section_2/Mistral-7B-finetuned_topk50_nb5_t1_HE.json`](data/section_2/Mistral-7B-finetuned_topk50_nb5_t1_HE.json)
- [`data/section_2/phi-2-finetuned_topk50_nb5_t1_HE.json`](data/section_2/phi-2-finetuned_topk50_nb5_t1_HE.json)

### Sample Human Annotation

```json
{
  "index": 1,
  "grammatical_correctness": 1,
  "coherence": 1,
  "correctness_of_answer": 1,
  "comment": "The response is grammatically correct, coherent, and provides a suitable answer to the question about exercise for arthritis."
}
```


## Hyperparameter Tuning

We explore the impact of different hyperparameters (top_k, beam_size, and temperature) on the text generation capabilities of the fine-tuned LLMs. We conduct experiments with varying parameter settings and measure their effects using the defined metrics.

Discussion - The presented data suggests that model performance is sensitive to parameter adjustments, but at different levels. Increasing top_k seems to influence semantic metrics such as BERTScore, introducing noise as more tokens are selected. Perplexity trends increase with more beams in one model, indicating greater uncertainty of prediction space. Surprisingly, temperature modifications do not show a significant effect, suggesting a potential robustness in the models or an insufficient test data to generate sufficient effects. These insights highlight the complex interplay between parameter tuning and model behavior, warranting further investigation to fully understand the underlying mechanisms.


![alt text](https://github.com/fatemehhaji/fine-tuning_LLMs/blob/main/images/parameters_evaluation.jpeg)

## References

- https://www.datacamp.com/tutorial/fine-tuning-llama-2
- https://www.kaggle.com/code/kingabzpro/fine-tuning-phi-2


