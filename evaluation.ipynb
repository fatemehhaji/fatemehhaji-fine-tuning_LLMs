{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import math\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_models = ['models/Llama-2-7b-hf-finetuned', 'models/Mistral-7B-finetuned', 'models/phi-2-finetuned']\n",
    "\n",
    "# Load your dataset\n",
    "dataset_name = \"tatsu-lab/alpaca\"  # Replace with your dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Select just 5 enteries \n",
    "# dataset = dataset['train'].shuffle(seed=42).select(range(5))\n",
    "\n",
    "# Split the dataset into train and test with a fixed seed\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.0005, seed=42)\n",
    "# train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, top_k, num_beams, temperature, test_dataset):\n",
    "    logging.set_verbosity_error()\n",
    "\n",
    "    # pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0,\n",
    "        top_k=top_k,  \n",
    "        num_beams=num_beams,  \n",
    "        temperature=temperature  \n",
    "    )\n",
    "\n",
    "    batch_size = 20\n",
    "\n",
    "    num_examples = len(test_dataset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "\n",
    "        # Generate text for the batch\n",
    "        results = pipe(prompts, max_new_tokens=128)\n",
    "        \n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text']\n",
    "            generated_output.append(generated_text)\n",
    "\n",
    "            # Uncomment the following lines if you want to print the prompts and generated text\n",
    "            prompt = prompts[results.index(result)]\n",
    "            # print(f\"Prompt: {prompt}\")\n",
    "            # print(f\"Generated Text: {generated_text}\")\n",
    "            # print(\"------\")\n",
    "    \n",
    "    return [output.split(\"### Response:\\n\")[1].split(\"\\n\\n### Instruction:\")[0].strip() if \"### Response:\\n\" in output else '' for output in generated_output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(model_name).eval()\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    if len(text) == 0:\n",
    "        print(f'THIS {text} RETURN ZERO')\n",
    "        return 0\n",
    "\n",
    "    tokenize_input = tokenizer_gpt2.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        loss = model_gpt2(tokenize_input, labels=tokenize_input)[0]\n",
    "\n",
    "    if not math.isnan(torch.exp(loss).item()):\n",
    "        return torch.exp(loss).item()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = [reference.split()]\n",
    "    candidate_tokens = candidate.split()\n",
    "    smoothie = SmoothingFunction().method1  # You can experiment with different smoothing methods\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "def calculate_rouge_l(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, candidate)['rougeL'].fmeasure\n",
    "\n",
    "def calculate_bert_score(reference, candidate):\n",
    "    *_, bert_scores = score([candidate], [reference], lang='en', return_hash=False)\n",
    "    return bert_scores.mean().item()\n",
    "\n",
    "def evaluate_text_quality(reference, candidate):\n",
    "    return {\n",
    "        'Perplexity': calculate_perplexity(candidate),\n",
    "        'BLEU': calculate_bleu(reference, candidate),\n",
    "        'ROUGE-L': calculate_rouge_l(reference, candidate),\n",
    "        'BERTScore': calculate_bert_score(reference, candidate)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# reference_text = \"This is a sample reference text.\"\n",
    "# generated_text = \"This is a sample generated text.\"\n",
    "# evaluation_results = evaluate_text_quality(reference_text, generated_text)\n",
    "# print(evaluation_results)\n",
    "\n",
    "\n",
    "def calculate_scores(test_dataset, generated_responses):\n",
    "    \"\"\"\n",
    "        Return the scores based on some generated text and the ground truth\n",
    "    \"\"\"\n",
    "    scores = {'Perplexity': 0, 'BLEU': 0, 'ROUGE-L': 0, 'BERTScore': 0}\n",
    "\n",
    "    num_samples = len(test_dataset)\n",
    "\n",
    "    for i, test_data in tqdm(enumerate(test_dataset)):\n",
    "        evaluation_results = evaluate_text_quality(test_data['output'], generated_responses[i])\n",
    "        for key in scores:\n",
    "            scores[key] += evaluation_results[key]\n",
    "\n",
    "    # Average the scores\n",
    "    for key in scores:\n",
    "        scores[key] /= num_samples\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "for each_model in finetuned_models:\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "    top_k = 50\n",
    "    num_beams = 5\n",
    "    temperature = 1\n",
    "    print(f'MODEL {each_model} START GENERATING.')\n",
    "    generated_responses = generate_response(model, tokenizer, top_k=top_k, num_beams=num_beams, temperature=temperature, test_dataset=test_dataset)\n",
    "\n",
    "    with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}.json', 'w+') as f:\n",
    "        json.dump(generated_responses, f)\n",
    "    print(f'MODEL {each_model} START CALCULATING SCORES.')\n",
    "    scores_model = calculate_scores(test_dataset=test_dataset, generated_responses=generated_responses)\n",
    "\n",
    "    with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}_score.json', 'w+') as f:\n",
    "        json.dump(scores_model, f)\n",
    "    print(scores_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values = [30, 40, 60, 80]\n",
    "beam_size_values = [2, 4, 6, 8]\n",
    "temperature_values = [0, 0.5, 0.7, 1]\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    for each_model in finetuned_models:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "        # top_k = 50\n",
    "        num_beams = 5\n",
    "        temperature = 1\n",
    "        print(f'MODEL {each_model} START GENERATING.')\n",
    "        generated_responses = generate_response(model, tokenizer, top_k=top_k, num_beams=num_beams, temperature=temperature, test_dataset=test_dataset)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}.json', 'w+') as f:\n",
    "            json.dump(generated_responses, f)\n",
    "        print(f'MODEL {each_model} START CALCULATING SCORES.')\n",
    "        scores_model = calculate_scores(test_dataset=test_dataset, generated_responses=generated_responses)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}_score.json', 'w+') as f:\n",
    "            json.dump(scores_model, f)\n",
    "        print(scores_model)\n",
    "\n",
    "\n",
    "for num_beams in beam_size_values:\n",
    "    for each_model in finetuned_models:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "        top_k = 50\n",
    "        # num_beams = 5\n",
    "        temperature = 1\n",
    "        print(f'MODEL {each_model} START GENERATING.')\n",
    "        generated_responses = generate_response(model, tokenizer, top_k=top_k, num_beams=num_beams, temperature=temperature, test_dataset=test_dataset)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}.json', 'w+') as f:\n",
    "            json.dump(generated_responses, f)\n",
    "        print(f'MODEL {each_model} START CALCULATING SCORES.')\n",
    "        scores_model = calculate_scores(test_dataset=test_dataset, generated_responses=generated_responses)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}_score.json', 'w+') as f:\n",
    "            json.dump(scores_model, f)\n",
    "        print(scores_model)\n",
    "\n",
    "\n",
    "for temperature in temperature_values:\n",
    "    for each_model in finetuned_models:\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "        top_k = 50\n",
    "        num_beams = 5\n",
    "        # temperature = 1\n",
    "        print(f'MODEL {each_model} START GENERATING.')\n",
    "        generated_responses = generate_response(model, tokenizer, top_k=top_k, num_beams=num_beams, temperature=temperature, test_dataset=test_dataset)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}.json', 'w+') as f:\n",
    "            json.dump(generated_responses, f)\n",
    "        print(f'MODEL {each_model} START CALCULATING SCORES.')\n",
    "        scores_model = calculate_scores(test_dataset=test_dataset, generated_responses=generated_responses)\n",
    "\n",
    "        with open(f'data/{each_model.split(\"/\")[1]}_topk{top_k}_nb{num_beams}_t{temperature}_score.json', 'w+') as f:\n",
    "            json.dump(scores_model, f)\n",
    "        print(scores_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
